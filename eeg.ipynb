{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "eeg.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnirudhDesai777/EEG_RNN/blob/main/eeg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KX-gL_eWvUYS",
        "outputId": "7c68f45b-dbe9-426d-aa2b-33f0e73f4918"
      },
      "source": [
        "#Intalling Pyeeg\n",
        "!pip install git+https://github.com/forrestbao/pyeeg.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/forrestbao/pyeeg.git\n",
            "  Cloning https://github.com/forrestbao/pyeeg.git to /tmp/pip-req-build-t94acugi\n",
            "  Running command git clone -q https://github.com/forrestbao/pyeeg.git /tmp/pip-req-build-t94acugi\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.7/dist-packages (from pyeeg==0.4.4) (1.19.5)\n",
            "Building wheels for collected packages: pyeeg\n",
            "  Building wheel for pyeeg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyeeg: filename=pyeeg-0.4.4-py2.py3-none-any.whl size=28132 sha256=43a02b86df9e8c956bbee7c5a7a5aaec9ad9cbfa7db73785c44c8191e31a4b24\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-lne0m2ew/wheels/b0/23/e4/703c908bda656959957029fa145879aa79307b2545a2ef0271\n",
            "Successfully built pyeeg\n",
            "Installing collected packages: pyeeg\n",
            "Successfully installed pyeeg-0.4.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iwtd-G8hvbrB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "26aa8fed-9fa2-4bb9-f44c-1925222c7c00"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import normalize\n",
        "import pickle as pickle\n",
        "import pandas as pd\n",
        "import pyeeg as pe\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.layers import LSTM,BatchNormalization,Activation\n",
        "from sklearn.model_selection import train_test_split\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.layers import LSTM,BatchNormalization,Activation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-61ce7033cef3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyeeg\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'to_categorical' from 'keras.utils' (/usr/local/lib/python3.7/dist-packages/keras/utils/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yo4ngkMmvsU8"
      },
      "source": [
        "#Mounting drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72i-9WAqgkjL"
      },
      "source": [
        "def getPSD(sub):\n",
        "    # channel = [1,2,3,4,6,11,13,17,19,20,21,25,29,31] #14 Channels chosen to fit Emotiv Epoch+\n",
        "    channel = [2, 3, 4, 6, 7, 11, 15, 17, 20, 24, 25, 27, 28, 29]\n",
        "    band = [4,8,12,16,25,45] #5 bands\n",
        "    window_size = 256 #Averaging band power of 2 sec\n",
        "    step_size = 16 #Each 0.125 sec update once\n",
        "    sample_rate = 128 #Sampling rate of 128 Hz\n",
        "    temp = []\n",
        "    with open(r'/content/drive/My Drive/data_preprocessed_python/s'+sub+'.dat','rb') as file:\n",
        "        \n",
        "        subject = pickle.load(file, encoding='latin1')\n",
        "        \n",
        "        for i in range(1,40):   # trials\n",
        "            raw_data = subject['data'][i]\n",
        "            raw_labels = subject['labels'][i]\n",
        "            start = 0\n",
        "            while start + window_size < len (raw_data[i]):\n",
        "                temp_array = []\n",
        "                temp_data = []\n",
        "                for j in channel:\n",
        "                    X = raw_data[j][start: start + window_size]\n",
        "                    Y = pe.bin_power(X, band, sample_rate)\n",
        "                    temp_data = temp_data + list(Y[0])\n",
        "                temp_array.append(np.array(temp_data))\n",
        "                temp_array.append(raw_labels)\n",
        "                temp.append(np.array(temp_array))\n",
        "                start = start + step_size\n",
        "        temp = np.array(temp)\n",
        "        np.save('out\\s' + sub, temp, allow_pickle=True, fix_imports=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwySLO7Lg0Qx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31e3f94d-6ded-4530-8f67-f776d3c852ce"
      },
      "source": [
        "subjectList=['01','02','03','04','05']\n",
        "# for i in range(0,32):\n",
        "#     if i < 10:\n",
        "#         subjectList.append('0' + str(i))\n",
        "#     else:\n",
        "#         subjectList.append(str(i))\n",
        "for sub in subjectList:\n",
        "    getPSD(sub)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGhbCnfFg2qd"
      },
      "source": [
        "data= []\n",
        "label = []\n",
        "for subjects in subjectList:\n",
        "  \n",
        "\n",
        "    with open('/content/out\\s' + subjects + '.npy', 'rb') as file:\n",
        "        sub = np.load(file,allow_pickle=True)\n",
        "        for i in range (0,sub.shape[0]):\n",
        "          data.append(sub[i][0])\n",
        "          label.append(sub[i][1])\n",
        "np.save('data', np.array(data), allow_pickle=True, fix_imports=True)\n",
        "np.save('label', np.array(label), allow_pickle=True, fix_imports=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQM-CNbIg-wD"
      },
      "source": [
        "df=pd.DataFrame(data=data)\n",
        "df.to_csv(\"data.csv\",index=False)\n",
        "\n",
        "df1=pd.DataFrame(data=label)\n",
        "df1.to_csv(\"label.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "KN3Py6vog_Xf",
        "outputId": "052f74b4-aaea-4979-934a-00b9a15e4571"
      },
      "source": [
        "data=pd.read_csv(\"data.csv\")\n",
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1192.392964</td>\n",
              "      <td>1291.832800</td>\n",
              "      <td>1381.865103</td>\n",
              "      <td>1475.910309</td>\n",
              "      <td>1363.050151</td>\n",
              "      <td>1130.469882</td>\n",
              "      <td>1590.203091</td>\n",
              "      <td>1084.023036</td>\n",
              "      <td>1625.203270</td>\n",
              "      <td>2404.378788</td>\n",
              "      <td>1061.119150</td>\n",
              "      <td>1393.873213</td>\n",
              "      <td>896.115949</td>\n",
              "      <td>1122.976710</td>\n",
              "      <td>1037.364226</td>\n",
              "      <td>656.258771</td>\n",
              "      <td>1904.388815</td>\n",
              "      <td>656.740472</td>\n",
              "      <td>1415.849605</td>\n",
              "      <td>1054.148973</td>\n",
              "      <td>1146.600522</td>\n",
              "      <td>1215.811983</td>\n",
              "      <td>849.723946</td>\n",
              "      <td>1569.081387</td>\n",
              "      <td>2257.047915</td>\n",
              "      <td>1254.827139</td>\n",
              "      <td>1488.309146</td>\n",
              "      <td>1194.016877</td>\n",
              "      <td>1332.751344</td>\n",
              "      <td>1646.312479</td>\n",
              "      <td>890.148983</td>\n",
              "      <td>2025.408959</td>\n",
              "      <td>1126.328309</td>\n",
              "      <td>836.309988</td>\n",
              "      <td>787.711535</td>\n",
              "      <td>1210.721769</td>\n",
              "      <td>1607.167643</td>\n",
              "      <td>1095.453455</td>\n",
              "      <td>1178.007491</td>\n",
              "      <td>1169.370107</td>\n",
              "      <td>777.126618</td>\n",
              "      <td>1738.700848</td>\n",
              "      <td>679.031798</td>\n",
              "      <td>1605.595767</td>\n",
              "      <td>1368.952679</td>\n",
              "      <td>662.640111</td>\n",
              "      <td>1101.724258</td>\n",
              "      <td>588.877396</td>\n",
              "      <td>1077.535332</td>\n",
              "      <td>971.611614</td>\n",
              "      <td>1113.585256</td>\n",
              "      <td>1634.656300</td>\n",
              "      <td>884.582666</td>\n",
              "      <td>1595.259345</td>\n",
              "      <td>1135.388245</td>\n",
              "      <td>555.120557</td>\n",
              "      <td>1617.339935</td>\n",
              "      <td>796.142902</td>\n",
              "      <td>1098.443243</td>\n",
              "      <td>927.336677</td>\n",
              "      <td>1493.421928</td>\n",
              "      <td>1987.889120</td>\n",
              "      <td>1180.709055</td>\n",
              "      <td>1271.477094</td>\n",
              "      <td>1545.076141</td>\n",
              "      <td>1284.809586</td>\n",
              "      <td>1850.756800</td>\n",
              "      <td>1121.058327</td>\n",
              "      <td>1035.231374</td>\n",
              "      <td>1177.191201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1138.623407</td>\n",
              "      <td>1425.736849</td>\n",
              "      <td>1197.125889</td>\n",
              "      <td>1602.427867</td>\n",
              "      <td>1298.423832</td>\n",
              "      <td>1079.579729</td>\n",
              "      <td>1796.748867</td>\n",
              "      <td>850.021234</td>\n",
              "      <td>1525.983627</td>\n",
              "      <td>2266.247815</td>\n",
              "      <td>998.910175</td>\n",
              "      <td>1453.404650</td>\n",
              "      <td>730.105746</td>\n",
              "      <td>1017.323851</td>\n",
              "      <td>919.850187</td>\n",
              "      <td>645.228847</td>\n",
              "      <td>1800.228017</td>\n",
              "      <td>591.238761</td>\n",
              "      <td>1314.672753</td>\n",
              "      <td>1020.852882</td>\n",
              "      <td>1096.243451</td>\n",
              "      <td>1526.903452</td>\n",
              "      <td>875.196482</td>\n",
              "      <td>1476.340832</td>\n",
              "      <td>2287.153522</td>\n",
              "      <td>1236.739727</td>\n",
              "      <td>1422.901131</td>\n",
              "      <td>1145.320258</td>\n",
              "      <td>1355.240962</td>\n",
              "      <td>1566.321322</td>\n",
              "      <td>822.680116</td>\n",
              "      <td>1977.980785</td>\n",
              "      <td>732.182879</td>\n",
              "      <td>763.430462</td>\n",
              "      <td>721.449412</td>\n",
              "      <td>1151.707736</td>\n",
              "      <td>1566.302448</td>\n",
              "      <td>837.979691</td>\n",
              "      <td>1249.403538</td>\n",
              "      <td>1247.264563</td>\n",
              "      <td>738.561749</td>\n",
              "      <td>1711.758782</td>\n",
              "      <td>581.359633</td>\n",
              "      <td>1539.216906</td>\n",
              "      <td>1314.426337</td>\n",
              "      <td>641.710807</td>\n",
              "      <td>1132.985826</td>\n",
              "      <td>630.638391</td>\n",
              "      <td>1133.344831</td>\n",
              "      <td>896.841713</td>\n",
              "      <td>1124.668287</td>\n",
              "      <td>1593.541525</td>\n",
              "      <td>829.792318</td>\n",
              "      <td>1533.849471</td>\n",
              "      <td>1060.774410</td>\n",
              "      <td>521.131605</td>\n",
              "      <td>1505.606776</td>\n",
              "      <td>722.033002</td>\n",
              "      <td>1021.196431</td>\n",
              "      <td>832.795736</td>\n",
              "      <td>1290.562132</td>\n",
              "      <td>1976.438509</td>\n",
              "      <td>995.926901</td>\n",
              "      <td>1259.550372</td>\n",
              "      <td>1436.532964</td>\n",
              "      <td>1202.235768</td>\n",
              "      <td>1788.538389</td>\n",
              "      <td>944.435522</td>\n",
              "      <td>988.151876</td>\n",
              "      <td>1130.179541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1106.175100</td>\n",
              "      <td>1331.414159</td>\n",
              "      <td>1210.214140</td>\n",
              "      <td>1814.696188</td>\n",
              "      <td>1247.438442</td>\n",
              "      <td>1096.400003</td>\n",
              "      <td>1576.065582</td>\n",
              "      <td>1019.326852</td>\n",
              "      <td>1800.861171</td>\n",
              "      <td>2206.700036</td>\n",
              "      <td>1055.653801</td>\n",
              "      <td>1257.597038</td>\n",
              "      <td>831.445721</td>\n",
              "      <td>1122.598021</td>\n",
              "      <td>929.022182</td>\n",
              "      <td>786.980958</td>\n",
              "      <td>1444.439552</td>\n",
              "      <td>811.150287</td>\n",
              "      <td>1378.520720</td>\n",
              "      <td>1066.411272</td>\n",
              "      <td>1091.177899</td>\n",
              "      <td>1483.078632</td>\n",
              "      <td>885.063235</td>\n",
              "      <td>1551.809936</td>\n",
              "      <td>2226.767242</td>\n",
              "      <td>1129.118980</td>\n",
              "      <td>1481.052779</td>\n",
              "      <td>1157.378545</td>\n",
              "      <td>1471.028025</td>\n",
              "      <td>1533.963514</td>\n",
              "      <td>865.957640</td>\n",
              "      <td>1891.259797</td>\n",
              "      <td>750.113423</td>\n",
              "      <td>904.066403</td>\n",
              "      <td>780.329085</td>\n",
              "      <td>1305.230219</td>\n",
              "      <td>1353.829600</td>\n",
              "      <td>789.279873</td>\n",
              "      <td>1347.708206</td>\n",
              "      <td>1255.483344</td>\n",
              "      <td>871.688399</td>\n",
              "      <td>1499.200512</td>\n",
              "      <td>644.869914</td>\n",
              "      <td>1537.343559</td>\n",
              "      <td>1250.145042</td>\n",
              "      <td>716.836749</td>\n",
              "      <td>995.211191</td>\n",
              "      <td>650.981871</td>\n",
              "      <td>1218.253120</td>\n",
              "      <td>902.370476</td>\n",
              "      <td>1117.349428</td>\n",
              "      <td>1419.688261</td>\n",
              "      <td>914.579199</td>\n",
              "      <td>1567.014226</td>\n",
              "      <td>1166.041608</td>\n",
              "      <td>569.531135</td>\n",
              "      <td>1430.852460</td>\n",
              "      <td>708.662986</td>\n",
              "      <td>1017.495506</td>\n",
              "      <td>773.648289</td>\n",
              "      <td>1287.918574</td>\n",
              "      <td>1877.849511</td>\n",
              "      <td>1082.550660</td>\n",
              "      <td>1306.000389</td>\n",
              "      <td>1466.979833</td>\n",
              "      <td>1086.534638</td>\n",
              "      <td>1586.070454</td>\n",
              "      <td>917.381182</td>\n",
              "      <td>1130.880391</td>\n",
              "      <td>1106.765751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1102.887845</td>\n",
              "      <td>1361.281167</td>\n",
              "      <td>1168.571156</td>\n",
              "      <td>1870.084156</td>\n",
              "      <td>1299.941771</td>\n",
              "      <td>1045.030642</td>\n",
              "      <td>1601.396406</td>\n",
              "      <td>1177.582939</td>\n",
              "      <td>1741.759993</td>\n",
              "      <td>2218.087229</td>\n",
              "      <td>931.764138</td>\n",
              "      <td>1320.869130</td>\n",
              "      <td>885.103664</td>\n",
              "      <td>1119.091679</td>\n",
              "      <td>957.868179</td>\n",
              "      <td>616.310301</td>\n",
              "      <td>1431.269620</td>\n",
              "      <td>607.623380</td>\n",
              "      <td>1280.587273</td>\n",
              "      <td>992.824063</td>\n",
              "      <td>1211.027900</td>\n",
              "      <td>1581.642325</td>\n",
              "      <td>869.268612</td>\n",
              "      <td>1534.609737</td>\n",
              "      <td>2294.653453</td>\n",
              "      <td>1451.726146</td>\n",
              "      <td>1736.254586</td>\n",
              "      <td>1453.387673</td>\n",
              "      <td>1459.315824</td>\n",
              "      <td>1498.112706</td>\n",
              "      <td>827.266182</td>\n",
              "      <td>1964.247506</td>\n",
              "      <td>822.407405</td>\n",
              "      <td>874.361065</td>\n",
              "      <td>720.490157</td>\n",
              "      <td>1379.889379</td>\n",
              "      <td>1343.959914</td>\n",
              "      <td>796.160950</td>\n",
              "      <td>1347.716628</td>\n",
              "      <td>1139.517245</td>\n",
              "      <td>868.833935</td>\n",
              "      <td>1304.247067</td>\n",
              "      <td>617.960793</td>\n",
              "      <td>1637.815779</td>\n",
              "      <td>1278.739714</td>\n",
              "      <td>768.150033</td>\n",
              "      <td>884.518390</td>\n",
              "      <td>709.681851</td>\n",
              "      <td>1170.075612</td>\n",
              "      <td>866.216208</td>\n",
              "      <td>1019.758135</td>\n",
              "      <td>1346.115882</td>\n",
              "      <td>795.179840</td>\n",
              "      <td>1516.018442</td>\n",
              "      <td>1112.529187</td>\n",
              "      <td>590.339682</td>\n",
              "      <td>1378.066783</td>\n",
              "      <td>736.828789</td>\n",
              "      <td>951.569769</td>\n",
              "      <td>764.212985</td>\n",
              "      <td>1287.916021</td>\n",
              "      <td>1890.251132</td>\n",
              "      <td>1005.982498</td>\n",
              "      <td>1340.215932</td>\n",
              "      <td>1349.773734</td>\n",
              "      <td>1084.075072</td>\n",
              "      <td>1543.237682</td>\n",
              "      <td>849.961940</td>\n",
              "      <td>1079.341565</td>\n",
              "      <td>1123.448290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1025.480047</td>\n",
              "      <td>1281.095804</td>\n",
              "      <td>1154.525974</td>\n",
              "      <td>1882.924231</td>\n",
              "      <td>1285.574104</td>\n",
              "      <td>1013.370234</td>\n",
              "      <td>1589.518527</td>\n",
              "      <td>1212.848401</td>\n",
              "      <td>1682.883079</td>\n",
              "      <td>2132.831929</td>\n",
              "      <td>941.585391</td>\n",
              "      <td>1450.873218</td>\n",
              "      <td>946.502853</td>\n",
              "      <td>1210.482247</td>\n",
              "      <td>884.182929</td>\n",
              "      <td>703.153969</td>\n",
              "      <td>1469.327131</td>\n",
              "      <td>720.996102</td>\n",
              "      <td>1295.906801</td>\n",
              "      <td>943.097498</td>\n",
              "      <td>1251.072484</td>\n",
              "      <td>1784.808854</td>\n",
              "      <td>897.921297</td>\n",
              "      <td>1560.190120</td>\n",
              "      <td>2319.578579</td>\n",
              "      <td>1296.458463</td>\n",
              "      <td>1894.352513</td>\n",
              "      <td>1459.802007</td>\n",
              "      <td>1427.654824</td>\n",
              "      <td>1549.767511</td>\n",
              "      <td>953.692815</td>\n",
              "      <td>1945.607592</td>\n",
              "      <td>879.791625</td>\n",
              "      <td>864.339146</td>\n",
              "      <td>727.429821</td>\n",
              "      <td>1265.371358</td>\n",
              "      <td>1400.953865</td>\n",
              "      <td>801.634491</td>\n",
              "      <td>1317.563218</td>\n",
              "      <td>1134.920635</td>\n",
              "      <td>901.386583</td>\n",
              "      <td>1281.923171</td>\n",
              "      <td>610.155415</td>\n",
              "      <td>1633.107287</td>\n",
              "      <td>1210.597981</td>\n",
              "      <td>763.299637</td>\n",
              "      <td>907.131790</td>\n",
              "      <td>681.992786</td>\n",
              "      <td>1186.118816</td>\n",
              "      <td>867.041586</td>\n",
              "      <td>1030.382659</td>\n",
              "      <td>1276.314140</td>\n",
              "      <td>890.151478</td>\n",
              "      <td>1566.164999</td>\n",
              "      <td>1093.272707</td>\n",
              "      <td>595.996633</td>\n",
              "      <td>1371.411424</td>\n",
              "      <td>718.524730</td>\n",
              "      <td>927.476385</td>\n",
              "      <td>769.504588</td>\n",
              "      <td>1255.114256</td>\n",
              "      <td>1840.630304</td>\n",
              "      <td>1009.903647</td>\n",
              "      <td>1353.798683</td>\n",
              "      <td>1345.206362</td>\n",
              "      <td>1121.149600</td>\n",
              "      <td>1444.011023</td>\n",
              "      <td>895.200534</td>\n",
              "      <td>1087.929197</td>\n",
              "      <td>1226.293997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95155</th>\n",
              "      <td>7460.450794</td>\n",
              "      <td>4093.623784</td>\n",
              "      <td>1795.917158</td>\n",
              "      <td>2741.759435</td>\n",
              "      <td>2686.213901</td>\n",
              "      <td>12359.065975</td>\n",
              "      <td>5858.852400</td>\n",
              "      <td>2618.190520</td>\n",
              "      <td>3413.336682</td>\n",
              "      <td>4089.299833</td>\n",
              "      <td>5773.673717</td>\n",
              "      <td>3122.602774</td>\n",
              "      <td>1561.704915</td>\n",
              "      <td>2102.847206</td>\n",
              "      <td>2347.192945</td>\n",
              "      <td>4198.799518</td>\n",
              "      <td>2411.809826</td>\n",
              "      <td>1366.405857</td>\n",
              "      <td>1837.462321</td>\n",
              "      <td>2078.500249</td>\n",
              "      <td>21334.521276</td>\n",
              "      <td>10483.305391</td>\n",
              "      <td>4073.464095</td>\n",
              "      <td>5478.549889</td>\n",
              "      <td>5807.507720</td>\n",
              "      <td>20322.043107</td>\n",
              "      <td>9758.332553</td>\n",
              "      <td>3460.665796</td>\n",
              "      <td>4673.303648</td>\n",
              "      <td>5301.944524</td>\n",
              "      <td>3468.981642</td>\n",
              "      <td>1927.567047</td>\n",
              "      <td>1175.923665</td>\n",
              "      <td>1586.357587</td>\n",
              "      <td>1616.955239</td>\n",
              "      <td>6313.706525</td>\n",
              "      <td>2703.345629</td>\n",
              "      <td>1163.041151</td>\n",
              "      <td>1048.478823</td>\n",
              "      <td>2011.761514</td>\n",
              "      <td>7146.818630</td>\n",
              "      <td>3379.303460</td>\n",
              "      <td>1747.754671</td>\n",
              "      <td>2209.597490</td>\n",
              "      <td>2614.903561</td>\n",
              "      <td>2086.787847</td>\n",
              "      <td>1033.479491</td>\n",
              "      <td>586.052851</td>\n",
              "      <td>611.372815</td>\n",
              "      <td>1146.093960</td>\n",
              "      <td>3155.061604</td>\n",
              "      <td>1633.853588</td>\n",
              "      <td>1112.964335</td>\n",
              "      <td>1678.742154</td>\n",
              "      <td>2019.088215</td>\n",
              "      <td>14435.860946</td>\n",
              "      <td>6478.138092</td>\n",
              "      <td>2253.552256</td>\n",
              "      <td>2420.013108</td>\n",
              "      <td>3920.904319</td>\n",
              "      <td>4258.757746</td>\n",
              "      <td>2274.701107</td>\n",
              "      <td>1516.908598</td>\n",
              "      <td>1945.539246</td>\n",
              "      <td>2056.483727</td>\n",
              "      <td>4915.631907</td>\n",
              "      <td>2447.181752</td>\n",
              "      <td>1398.921958</td>\n",
              "      <td>1701.585083</td>\n",
              "      <td>1815.481334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95156</th>\n",
              "      <td>6442.904720</td>\n",
              "      <td>3345.487443</td>\n",
              "      <td>2134.100846</td>\n",
              "      <td>3046.230566</td>\n",
              "      <td>2727.886853</td>\n",
              "      <td>9835.715580</td>\n",
              "      <td>5178.084820</td>\n",
              "      <td>2906.757859</td>\n",
              "      <td>3953.176573</td>\n",
              "      <td>3992.634713</td>\n",
              "      <td>5189.314462</td>\n",
              "      <td>2732.751285</td>\n",
              "      <td>1747.653464</td>\n",
              "      <td>2314.713382</td>\n",
              "      <td>2494.338512</td>\n",
              "      <td>3972.987924</td>\n",
              "      <td>1966.974376</td>\n",
              "      <td>1610.534102</td>\n",
              "      <td>1891.662884</td>\n",
              "      <td>2364.916654</td>\n",
              "      <td>16685.016246</td>\n",
              "      <td>8851.098826</td>\n",
              "      <td>4740.101449</td>\n",
              "      <td>6514.575988</td>\n",
              "      <td>5632.691464</td>\n",
              "      <td>15635.611905</td>\n",
              "      <td>7924.636632</td>\n",
              "      <td>4062.245744</td>\n",
              "      <td>5577.525455</td>\n",
              "      <td>4997.531393</td>\n",
              "      <td>3359.184411</td>\n",
              "      <td>1727.211228</td>\n",
              "      <td>1232.424671</td>\n",
              "      <td>1670.917600</td>\n",
              "      <td>1755.822065</td>\n",
              "      <td>4857.560548</td>\n",
              "      <td>2247.169916</td>\n",
              "      <td>1301.790669</td>\n",
              "      <td>1553.252060</td>\n",
              "      <td>1752.392067</td>\n",
              "      <td>6199.602736</td>\n",
              "      <td>2990.344699</td>\n",
              "      <td>1952.530946</td>\n",
              "      <td>2593.676222</td>\n",
              "      <td>2588.725706</td>\n",
              "      <td>1451.944325</td>\n",
              "      <td>852.967907</td>\n",
              "      <td>647.161595</td>\n",
              "      <td>580.588045</td>\n",
              "      <td>1054.029209</td>\n",
              "      <td>2877.660519</td>\n",
              "      <td>1396.629990</td>\n",
              "      <td>1223.062504</td>\n",
              "      <td>1582.664932</td>\n",
              "      <td>2202.945727</td>\n",
              "      <td>11020.001365</td>\n",
              "      <td>5291.786382</td>\n",
              "      <td>2482.054883</td>\n",
              "      <td>3233.905779</td>\n",
              "      <td>3526.040808</td>\n",
              "      <td>4130.856506</td>\n",
              "      <td>2035.894267</td>\n",
              "      <td>1639.012998</td>\n",
              "      <td>2010.803096</td>\n",
              "      <td>2314.413739</td>\n",
              "      <td>4250.309815</td>\n",
              "      <td>2044.465578</td>\n",
              "      <td>1552.559943</td>\n",
              "      <td>1850.219291</td>\n",
              "      <td>1878.232025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95157</th>\n",
              "      <td>5446.387128</td>\n",
              "      <td>2641.438719</td>\n",
              "      <td>1832.588798</td>\n",
              "      <td>2635.235247</td>\n",
              "      <td>2737.164753</td>\n",
              "      <td>7821.842944</td>\n",
              "      <td>3810.077600</td>\n",
              "      <td>2330.180418</td>\n",
              "      <td>3438.576069</td>\n",
              "      <td>4389.206304</td>\n",
              "      <td>4647.139009</td>\n",
              "      <td>2211.233533</td>\n",
              "      <td>1484.812971</td>\n",
              "      <td>2022.697039</td>\n",
              "      <td>2337.480261</td>\n",
              "      <td>3795.208395</td>\n",
              "      <td>1803.141752</td>\n",
              "      <td>1426.349390</td>\n",
              "      <td>1894.827496</td>\n",
              "      <td>2131.526034</td>\n",
              "      <td>12695.429759</td>\n",
              "      <td>6351.799313</td>\n",
              "      <td>3917.155655</td>\n",
              "      <td>5626.502286</td>\n",
              "      <td>6062.758247</td>\n",
              "      <td>10729.742503</td>\n",
              "      <td>5506.946306</td>\n",
              "      <td>3265.801623</td>\n",
              "      <td>4972.520581</td>\n",
              "      <td>5589.126673</td>\n",
              "      <td>3190.531403</td>\n",
              "      <td>1547.787628</td>\n",
              "      <td>990.162954</td>\n",
              "      <td>1503.280521</td>\n",
              "      <td>1564.744590</td>\n",
              "      <td>3077.846820</td>\n",
              "      <td>1560.151903</td>\n",
              "      <td>928.419112</td>\n",
              "      <td>1207.784187</td>\n",
              "      <td>1882.187078</td>\n",
              "      <td>5288.225495</td>\n",
              "      <td>2457.884417</td>\n",
              "      <td>1519.064054</td>\n",
              "      <td>2145.011064</td>\n",
              "      <td>2550.717316</td>\n",
              "      <td>845.920708</td>\n",
              "      <td>561.149291</td>\n",
              "      <td>615.083969</td>\n",
              "      <td>588.738134</td>\n",
              "      <td>1077.947353</td>\n",
              "      <td>2618.347441</td>\n",
              "      <td>1405.195195</td>\n",
              "      <td>1121.220062</td>\n",
              "      <td>1732.586375</td>\n",
              "      <td>2048.379768</td>\n",
              "      <td>6648.354140</td>\n",
              "      <td>3513.561714</td>\n",
              "      <td>1801.133750</td>\n",
              "      <td>2906.241397</td>\n",
              "      <td>3991.539085</td>\n",
              "      <td>3955.447495</td>\n",
              "      <td>1889.088748</td>\n",
              "      <td>1408.346194</td>\n",
              "      <td>1937.126220</td>\n",
              "      <td>2089.349529</td>\n",
              "      <td>3736.446178</td>\n",
              "      <td>1561.289572</td>\n",
              "      <td>1355.046599</td>\n",
              "      <td>1725.462950</td>\n",
              "      <td>1869.785278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95158</th>\n",
              "      <td>5423.014216</td>\n",
              "      <td>2671.897630</td>\n",
              "      <td>1873.809694</td>\n",
              "      <td>2563.345491</td>\n",
              "      <td>2585.836207</td>\n",
              "      <td>7620.533884</td>\n",
              "      <td>3838.556883</td>\n",
              "      <td>2369.365996</td>\n",
              "      <td>3248.526907</td>\n",
              "      <td>3739.816164</td>\n",
              "      <td>4598.411123</td>\n",
              "      <td>2313.351283</td>\n",
              "      <td>1526.006928</td>\n",
              "      <td>2021.676396</td>\n",
              "      <td>2271.735828</td>\n",
              "      <td>3789.876755</td>\n",
              "      <td>1785.864463</td>\n",
              "      <td>1461.370866</td>\n",
              "      <td>1908.005952</td>\n",
              "      <td>2128.314142</td>\n",
              "      <td>12390.641836</td>\n",
              "      <td>6245.616577</td>\n",
              "      <td>3983.273519</td>\n",
              "      <td>5110.899611</td>\n",
              "      <td>5178.182137</td>\n",
              "      <td>10411.200204</td>\n",
              "      <td>5246.037178</td>\n",
              "      <td>3315.987831</td>\n",
              "      <td>4263.036440</td>\n",
              "      <td>4762.633424</td>\n",
              "      <td>3202.617048</td>\n",
              "      <td>1561.061867</td>\n",
              "      <td>977.483770</td>\n",
              "      <td>1530.485597</td>\n",
              "      <td>1573.607046</td>\n",
              "      <td>2777.971361</td>\n",
              "      <td>1405.907048</td>\n",
              "      <td>874.367499</td>\n",
              "      <td>1085.366099</td>\n",
              "      <td>1719.258842</td>\n",
              "      <td>5230.575442</td>\n",
              "      <td>2470.969952</td>\n",
              "      <td>1513.395243</td>\n",
              "      <td>2065.486898</td>\n",
              "      <td>2477.741877</td>\n",
              "      <td>796.930677</td>\n",
              "      <td>530.328617</td>\n",
              "      <td>642.854633</td>\n",
              "      <td>519.371228</td>\n",
              "      <td>1054.653634</td>\n",
              "      <td>2617.635849</td>\n",
              "      <td>1372.217759</td>\n",
              "      <td>1135.980253</td>\n",
              "      <td>1682.213881</td>\n",
              "      <td>2041.219072</td>\n",
              "      <td>6194.848116</td>\n",
              "      <td>3142.843017</td>\n",
              "      <td>1759.817819</td>\n",
              "      <td>2191.704095</td>\n",
              "      <td>3291.272040</td>\n",
              "      <td>3951.434363</td>\n",
              "      <td>1903.183893</td>\n",
              "      <td>1423.915727</td>\n",
              "      <td>1964.458309</td>\n",
              "      <td>2076.938980</td>\n",
              "      <td>3667.379804</td>\n",
              "      <td>1659.504668</td>\n",
              "      <td>1398.805807</td>\n",
              "      <td>1676.185599</td>\n",
              "      <td>1783.300325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95159</th>\n",
              "      <td>5253.930076</td>\n",
              "      <td>2617.037606</td>\n",
              "      <td>1833.872614</td>\n",
              "      <td>2620.171978</td>\n",
              "      <td>2687.259753</td>\n",
              "      <td>7298.197257</td>\n",
              "      <td>3692.769558</td>\n",
              "      <td>2351.418566</td>\n",
              "      <td>3262.546726</td>\n",
              "      <td>4004.488128</td>\n",
              "      <td>4428.180948</td>\n",
              "      <td>2244.603017</td>\n",
              "      <td>1519.118199</td>\n",
              "      <td>2046.139875</td>\n",
              "      <td>2393.273178</td>\n",
              "      <td>3680.894093</td>\n",
              "      <td>1679.063498</td>\n",
              "      <td>1508.304870</td>\n",
              "      <td>1960.741214</td>\n",
              "      <td>2188.768739</td>\n",
              "      <td>11836.295380</td>\n",
              "      <td>6005.590937</td>\n",
              "      <td>4044.119810</td>\n",
              "      <td>5172.334740</td>\n",
              "      <td>5390.920968</td>\n",
              "      <td>9988.601113</td>\n",
              "      <td>5063.290285</td>\n",
              "      <td>3388.310986</td>\n",
              "      <td>4338.386038</td>\n",
              "      <td>4994.174285</td>\n",
              "      <td>3160.129018</td>\n",
              "      <td>1571.959611</td>\n",
              "      <td>934.322346</td>\n",
              "      <td>1529.920598</td>\n",
              "      <td>1637.561589</td>\n",
              "      <td>2745.638738</td>\n",
              "      <td>1340.740222</td>\n",
              "      <td>771.247790</td>\n",
              "      <td>1102.144493</td>\n",
              "      <td>1714.044865</td>\n",
              "      <td>5099.024733</td>\n",
              "      <td>2373.523436</td>\n",
              "      <td>1517.783934</td>\n",
              "      <td>2115.234947</td>\n",
              "      <td>2403.286293</td>\n",
              "      <td>761.507000</td>\n",
              "      <td>578.462557</td>\n",
              "      <td>655.897104</td>\n",
              "      <td>548.191446</td>\n",
              "      <td>1088.597047</td>\n",
              "      <td>2603.414277</td>\n",
              "      <td>1437.047055</td>\n",
              "      <td>1234.782353</td>\n",
              "      <td>1742.849597</td>\n",
              "      <td>2076.045801</td>\n",
              "      <td>5965.187347</td>\n",
              "      <td>3053.360203</td>\n",
              "      <td>1723.798999</td>\n",
              "      <td>2222.802787</td>\n",
              "      <td>3444.418593</td>\n",
              "      <td>3888.938648</td>\n",
              "      <td>1839.543275</td>\n",
              "      <td>1435.804706</td>\n",
              "      <td>1962.831930</td>\n",
              "      <td>2085.415080</td>\n",
              "      <td>3535.957378</td>\n",
              "      <td>1534.812549</td>\n",
              "      <td>1424.594725</td>\n",
              "      <td>1675.634635</td>\n",
              "      <td>1848.353565</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>95160 rows × 70 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 0            1  ...           68           69\n",
              "0      1192.392964  1291.832800  ...  1035.231374  1177.191201\n",
              "1      1138.623407  1425.736849  ...   988.151876  1130.179541\n",
              "2      1106.175100  1331.414159  ...  1130.880391  1106.765751\n",
              "3      1102.887845  1361.281167  ...  1079.341565  1123.448290\n",
              "4      1025.480047  1281.095804  ...  1087.929197  1226.293997\n",
              "...            ...          ...  ...          ...          ...\n",
              "95155  7460.450794  4093.623784  ...  1701.585083  1815.481334\n",
              "95156  6442.904720  3345.487443  ...  1850.219291  1878.232025\n",
              "95157  5446.387128  2641.438719  ...  1725.462950  1869.785278\n",
              "95158  5423.014216  2671.897630  ...  1676.185599  1783.300325\n",
              "95159  5253.930076  2617.037606  ...  1675.634635  1848.353565\n",
              "\n",
              "[95160 rows x 70 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "-ltPE1TYyuco",
        "outputId": "bae807a6-7ee3-4b56-94c6-cfeabf3bbd18"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(data)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "Tvu_bUN6hCPF",
        "outputId": "b35347fe-741f-4820-fa2f-5cad04f17036"
      },
      "source": [
        "label=pd.read_csv(\"label.csv\")\n",
        "label\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8.1</td>\n",
              "      <td>7.31</td>\n",
              "      <td>7.28</td>\n",
              "      <td>8.47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8.1</td>\n",
              "      <td>7.31</td>\n",
              "      <td>7.28</td>\n",
              "      <td>8.47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8.1</td>\n",
              "      <td>7.31</td>\n",
              "      <td>7.28</td>\n",
              "      <td>8.47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8.1</td>\n",
              "      <td>7.31</td>\n",
              "      <td>7.28</td>\n",
              "      <td>8.47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8.1</td>\n",
              "      <td>7.31</td>\n",
              "      <td>7.28</td>\n",
              "      <td>8.47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95155</th>\n",
              "      <td>7.0</td>\n",
              "      <td>6.96</td>\n",
              "      <td>8.00</td>\n",
              "      <td>2.92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95156</th>\n",
              "      <td>7.0</td>\n",
              "      <td>6.96</td>\n",
              "      <td>8.00</td>\n",
              "      <td>2.92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95157</th>\n",
              "      <td>7.0</td>\n",
              "      <td>6.96</td>\n",
              "      <td>8.00</td>\n",
              "      <td>2.92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95158</th>\n",
              "      <td>7.0</td>\n",
              "      <td>6.96</td>\n",
              "      <td>8.00</td>\n",
              "      <td>2.92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95159</th>\n",
              "      <td>7.0</td>\n",
              "      <td>6.96</td>\n",
              "      <td>8.00</td>\n",
              "      <td>2.92</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>95160 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         0     1     2     3\n",
              "0      8.1  7.31  7.28  8.47\n",
              "1      8.1  7.31  7.28  8.47\n",
              "2      8.1  7.31  7.28  8.47\n",
              "3      8.1  7.31  7.28  8.47\n",
              "4      8.1  7.31  7.28  8.47\n",
              "...    ...   ...   ...   ...\n",
              "95155  7.0  6.96  8.00  2.92\n",
              "95156  7.0  6.96  8.00  2.92\n",
              "95157  7.0  6.96  8.00  2.92\n",
              "95158  7.0  6.96  8.00  2.92\n",
              "95159  7.0  6.96  8.00  2.92\n",
              "\n",
              "[95160 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "NWFUQrMuhFej",
        "outputId": "1a72a472-f60e-4a3d-ecd7-fc04c12bf870"
      },
      "source": [
        "y_valence=label.loc[:,'0']\n",
        "plt.hist(y_valence)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "jCuU8szqPuG_",
        "outputId": "c5965acd-7ad4-40b4-b410-cd6de98e7c0b"
      },
      "source": [
        "y_arousal=label.loc[:,'1']\n",
        "plt.hist(y_arousal)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GtSXJDZhIhn",
        "outputId": "ffbef220-ef33-458b-d163-1ff16ab6ac55"
      },
      "source": [
        "# normalize the dataset\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "x=data\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "x = scaler.fit_transform(x)\n",
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.01868262, 0.04674496, 0.10241176, ..., 0.10855983, 0.04119163,\n",
              "        0.01250635],\n",
              "       [0.01755441, 0.05299384, 0.0865895 , ..., 0.08718907, 0.03748492,\n",
              "        0.01118327],\n",
              "       [0.01687357, 0.04859209, 0.08771047, ..., 0.08391559, 0.04872236,\n",
              "        0.01052432],\n",
              "       ...,\n",
              "       [0.10794128, 0.10972687, 0.14101464, ..., 0.13687162, 0.09553557,\n",
              "        0.03199851],\n",
              "       [0.10745087, 0.1111483 , 0.14454506, ..., 0.14216634, 0.09165582,\n",
              "        0.0295645 ],\n",
              "       [0.10390309, 0.10858815, 0.1411246 , ..., 0.14528671, 0.09161244,\n",
              "        0.03139534]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSOKz34bQWd8",
        "outputId": "ddecc267-313b-48fa-9f61-26548d061f2d"
      },
      "source": [
        "y_arousal"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        7.31\n",
              "1        7.31\n",
              "2        7.31\n",
              "3        7.31\n",
              "4        7.31\n",
              "         ... \n",
              "95155    6.96\n",
              "95156    6.96\n",
              "95157    6.96\n",
              "95158    6.96\n",
              "95159    6.96\n",
              "Name: 1, Length: 95160, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yC7XcCzxQITt",
        "outputId": "7fbfc5bb-3781-4b2d-add8-43831c122400"
      },
      "source": [
        "y_val = to_categorical(y_valence)\n",
        "y_val.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(95160, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zT0lgvlKQS1M",
        "outputId": "8ac91076-37de-4888-d9c2-8a576385a7f3"
      },
      "source": [
        "y_aro = to_categorical(y_arousal)\n",
        "y_aro"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 1., 0., 0.],\n",
              "       [0., 0., 0., ..., 1., 0., 0.],\n",
              "       [0., 0., 0., ..., 1., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cjn1ttYhJLt"
      },
      "source": [
        "x = np.reshape(x, (x.shape[0],1,x.shape[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXBa5MelhLot"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x, y_val, test_size = 0.2, random_state = 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y779wyeoVUqv",
        "outputId": "6c690747-c4e2-4221-93f0-9da898160dc6"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(76128, 1, 70)\n",
            "(76128, 10)\n",
            "(19032, 1, 70)\n",
            "(19032, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LX4E3DNahOn_",
        "outputId": "573bbc0b-267f-44a9-8a63-b755a7ff9f81"
      },
      "source": [
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(512, batch_input_shape = (None, None, x.shape[2]),return_sequences=True))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "\n",
        "model.add(LSTM(256,activation=\"relu\",return_sequences=True))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "\n",
        "model.add(LSTM(128,activation=\"relu\",return_sequences=True))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(LSTM(64,activation=\"relu\",return_sequences=True))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "\n",
        "model.add(LSTM(32,activation=\"relu\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "\n",
        "rmsprop =keras.optimizers.RMSprop(lr=0.0005, rho=0.9, epsilon=1e-08)\n",
        "model.compile(loss='mean_squared_error',\n",
        "                  optimizer=rmsprop,\n",
        "                  metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, None, 512)         1193984   \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, None, 512)         2048      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, None, 512)         0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, None, 256)         787456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, None, 256)         1024      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, None, 256)         0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, None, 128)         197120    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, None, 128)         512       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, None, 128)         0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, None, 64)          49408     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, None, 64)          256       \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, None, 64)          0         \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 32)                12416     \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                330       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 2,244,682\n",
            "Trainable params: 2,242,698\n",
            "Non-trainable params: 1,984\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDBrSSxfhRd3",
        "outputId": "384eb408-738e-46b6-a4f6-9587cec4a154"
      },
      "source": [
        "history = model.fit(x_train, y_train, epochs = 100, batch_size=150,validation_data= (x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "508/508 [==============================] - 24s 22ms/step - loss: 0.0940 - accuracy: 0.1611 - val_loss: 0.0876 - val_accuracy: 0.1812\n",
            "Epoch 2/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0854 - accuracy: 0.2352 - val_loss: 0.0816 - val_accuracy: 0.3019\n",
            "Epoch 3/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0829 - accuracy: 0.2694 - val_loss: 0.0796 - val_accuracy: 0.3183\n",
            "Epoch 4/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0817 - accuracy: 0.2879 - val_loss: 0.0791 - val_accuracy: 0.3222\n",
            "Epoch 5/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0806 - accuracy: 0.3060 - val_loss: 0.0782 - val_accuracy: 0.3312\n",
            "Epoch 6/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0796 - accuracy: 0.3190 - val_loss: 0.0774 - val_accuracy: 0.3431\n",
            "Epoch 7/100\n",
            "508/508 [==============================] - 11s 21ms/step - loss: 0.0788 - accuracy: 0.3325 - val_loss: 0.0765 - val_accuracy: 0.3536\n",
            "Epoch 8/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0780 - accuracy: 0.3416 - val_loss: 0.0748 - val_accuracy: 0.3750\n",
            "Epoch 9/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0772 - accuracy: 0.3509 - val_loss: 0.0745 - val_accuracy: 0.3753\n",
            "Epoch 10/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0767 - accuracy: 0.3606 - val_loss: 0.0746 - val_accuracy: 0.3793\n",
            "Epoch 11/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0761 - accuracy: 0.3655 - val_loss: 0.0726 - val_accuracy: 0.3977\n",
            "Epoch 12/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0756 - accuracy: 0.3736 - val_loss: 0.0735 - val_accuracy: 0.3974\n",
            "Epoch 13/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0749 - accuracy: 0.3814 - val_loss: 0.0725 - val_accuracy: 0.4045\n",
            "Epoch 14/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0744 - accuracy: 0.3880 - val_loss: 0.0701 - val_accuracy: 0.4337\n",
            "Epoch 15/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0737 - accuracy: 0.3982 - val_loss: 0.0707 - val_accuracy: 0.4264\n",
            "Epoch 16/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0736 - accuracy: 0.3998 - val_loss: 0.0697 - val_accuracy: 0.4397\n",
            "Epoch 17/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0729 - accuracy: 0.4086 - val_loss: 0.0690 - val_accuracy: 0.4444\n",
            "Epoch 18/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0723 - accuracy: 0.4148 - val_loss: 0.0690 - val_accuracy: 0.4448\n",
            "Epoch 19/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0718 - accuracy: 0.4176 - val_loss: 0.0677 - val_accuracy: 0.4611\n",
            "Epoch 20/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0716 - accuracy: 0.4248 - val_loss: 0.0669 - val_accuracy: 0.4651\n",
            "Epoch 21/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0711 - accuracy: 0.4285 - val_loss: 0.0670 - val_accuracy: 0.4692\n",
            "Epoch 22/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0706 - accuracy: 0.4335 - val_loss: 0.0654 - val_accuracy: 0.4847\n",
            "Epoch 23/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0703 - accuracy: 0.4376 - val_loss: 0.0654 - val_accuracy: 0.4822\n",
            "Epoch 24/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0700 - accuracy: 0.4429 - val_loss: 0.0642 - val_accuracy: 0.4951\n",
            "Epoch 25/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0694 - accuracy: 0.4469 - val_loss: 0.0646 - val_accuracy: 0.4970\n",
            "Epoch 26/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0689 - accuracy: 0.4532 - val_loss: 0.0641 - val_accuracy: 0.4943\n",
            "Epoch 27/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0687 - accuracy: 0.4562 - val_loss: 0.0648 - val_accuracy: 0.4901\n",
            "Epoch 28/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0683 - accuracy: 0.4603 - val_loss: 0.0624 - val_accuracy: 0.5144\n",
            "Epoch 29/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0679 - accuracy: 0.4655 - val_loss: 0.0643 - val_accuracy: 0.5013\n",
            "Epoch 30/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0675 - accuracy: 0.4710 - val_loss: 0.0616 - val_accuracy: 0.5207\n",
            "Epoch 31/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0674 - accuracy: 0.4689 - val_loss: 0.0618 - val_accuracy: 0.5205\n",
            "Epoch 32/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0667 - accuracy: 0.4781 - val_loss: 0.0609 - val_accuracy: 0.5257\n",
            "Epoch 33/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0669 - accuracy: 0.4761 - val_loss: 0.0607 - val_accuracy: 0.5299\n",
            "Epoch 34/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0664 - accuracy: 0.4813 - val_loss: 0.0600 - val_accuracy: 0.5410\n",
            "Epoch 35/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0660 - accuracy: 0.4875 - val_loss: 0.0587 - val_accuracy: 0.5495\n",
            "Epoch 36/100\n",
            "508/508 [==============================] - 11s 21ms/step - loss: 0.0659 - accuracy: 0.4870 - val_loss: 0.0589 - val_accuracy: 0.5485\n",
            "Epoch 37/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0653 - accuracy: 0.4924 - val_loss: 0.0583 - val_accuracy: 0.5544\n",
            "Epoch 38/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0652 - accuracy: 0.4967 - val_loss: 0.0585 - val_accuracy: 0.5492\n",
            "Epoch 39/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0644 - accuracy: 0.5005 - val_loss: 0.0579 - val_accuracy: 0.5534\n",
            "Epoch 40/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0643 - accuracy: 0.5020 - val_loss: 0.0575 - val_accuracy: 0.5629\n",
            "Epoch 41/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0641 - accuracy: 0.5043 - val_loss: 0.0582 - val_accuracy: 0.5546\n",
            "Epoch 42/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0638 - accuracy: 0.5076 - val_loss: 0.0573 - val_accuracy: 0.5623\n",
            "Epoch 43/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0637 - accuracy: 0.5082 - val_loss: 0.0559 - val_accuracy: 0.5768\n",
            "Epoch 44/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0632 - accuracy: 0.5131 - val_loss: 0.0569 - val_accuracy: 0.5614\n",
            "Epoch 45/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0629 - accuracy: 0.5192 - val_loss: 0.0565 - val_accuracy: 0.5719\n",
            "Epoch 46/100\n",
            "508/508 [==============================] - 10s 21ms/step - loss: 0.0628 - accuracy: 0.5215 - val_loss: 0.0561 - val_accuracy: 0.5756\n",
            "Epoch 47/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0623 - accuracy: 0.5188 - val_loss: 0.0554 - val_accuracy: 0.5847\n",
            "Epoch 48/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0625 - accuracy: 0.5217 - val_loss: 0.0561 - val_accuracy: 0.5803\n",
            "Epoch 49/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0621 - accuracy: 0.5263 - val_loss: 0.0534 - val_accuracy: 0.5964\n",
            "Epoch 50/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0615 - accuracy: 0.5316 - val_loss: 0.0533 - val_accuracy: 0.6001\n",
            "Epoch 51/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0612 - accuracy: 0.5344 - val_loss: 0.0536 - val_accuracy: 0.5961\n",
            "Epoch 52/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0611 - accuracy: 0.5347 - val_loss: 0.0530 - val_accuracy: 0.6004\n",
            "Epoch 53/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0607 - accuracy: 0.5382 - val_loss: 0.0532 - val_accuracy: 0.5974\n",
            "Epoch 54/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0608 - accuracy: 0.5386 - val_loss: 0.0533 - val_accuracy: 0.5998\n",
            "Epoch 55/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0604 - accuracy: 0.5412 - val_loss: 0.0523 - val_accuracy: 0.6041\n",
            "Epoch 56/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0599 - accuracy: 0.5457 - val_loss: 0.0522 - val_accuracy: 0.6106\n",
            "Epoch 57/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0604 - accuracy: 0.5410 - val_loss: 0.0521 - val_accuracy: 0.6086\n",
            "Epoch 58/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0602 - accuracy: 0.5447 - val_loss: 0.0508 - val_accuracy: 0.6220\n",
            "Epoch 59/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0592 - accuracy: 0.5540 - val_loss: 0.0512 - val_accuracy: 0.6190\n",
            "Epoch 60/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0593 - accuracy: 0.5517 - val_loss: 0.0517 - val_accuracy: 0.6162\n",
            "Epoch 61/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0590 - accuracy: 0.5567 - val_loss: 0.0519 - val_accuracy: 0.6129\n",
            "Epoch 62/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0589 - accuracy: 0.5587 - val_loss: 0.0494 - val_accuracy: 0.6344\n",
            "Epoch 63/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0587 - accuracy: 0.5547 - val_loss: 0.0493 - val_accuracy: 0.6349\n",
            "Epoch 64/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0586 - accuracy: 0.5597 - val_loss: 0.0521 - val_accuracy: 0.6084\n",
            "Epoch 65/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0584 - accuracy: 0.5598 - val_loss: 0.0492 - val_accuracy: 0.6352\n",
            "Epoch 66/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0582 - accuracy: 0.5673 - val_loss: 0.0492 - val_accuracy: 0.6327\n",
            "Epoch 67/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0575 - accuracy: 0.5671 - val_loss: 0.0502 - val_accuracy: 0.6269\n",
            "Epoch 68/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0581 - accuracy: 0.5650 - val_loss: 0.0489 - val_accuracy: 0.6308\n",
            "Epoch 69/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0580 - accuracy: 0.5657 - val_loss: 0.0521 - val_accuracy: 0.6118\n",
            "Epoch 70/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0574 - accuracy: 0.5709 - val_loss: 0.0472 - val_accuracy: 0.6483\n",
            "Epoch 71/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0572 - accuracy: 0.5732 - val_loss: 0.0468 - val_accuracy: 0.6534\n",
            "Epoch 72/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0574 - accuracy: 0.5696 - val_loss: 0.0478 - val_accuracy: 0.6459\n",
            "Epoch 73/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0568 - accuracy: 0.5743 - val_loss: 0.0481 - val_accuracy: 0.6411\n",
            "Epoch 74/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0569 - accuracy: 0.5747 - val_loss: 0.0463 - val_accuracy: 0.6576\n",
            "Epoch 75/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0566 - accuracy: 0.5758 - val_loss: 0.0461 - val_accuracy: 0.6609\n",
            "Epoch 76/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0565 - accuracy: 0.5795 - val_loss: 0.0459 - val_accuracy: 0.6629\n",
            "Epoch 77/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0560 - accuracy: 0.5861 - val_loss: 0.0452 - val_accuracy: 0.6655\n",
            "Epoch 78/100\n",
            "508/508 [==============================] - 10s 21ms/step - loss: 0.0559 - accuracy: 0.5845 - val_loss: 0.0463 - val_accuracy: 0.6607\n",
            "Epoch 79/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0559 - accuracy: 0.5845 - val_loss: 0.0460 - val_accuracy: 0.6613\n",
            "Epoch 80/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0559 - accuracy: 0.5833 - val_loss: 0.0456 - val_accuracy: 0.6641\n",
            "Epoch 81/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0554 - accuracy: 0.5856 - val_loss: 0.0501 - val_accuracy: 0.6271\n",
            "Epoch 82/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0552 - accuracy: 0.5894 - val_loss: 0.0448 - val_accuracy: 0.6683\n",
            "Epoch 83/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0553 - accuracy: 0.5889 - val_loss: 0.0441 - val_accuracy: 0.6745\n",
            "Epoch 84/100\n",
            "508/508 [==============================] - 10s 21ms/step - loss: 0.0547 - accuracy: 0.5942 - val_loss: 0.0449 - val_accuracy: 0.6677\n",
            "Epoch 85/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0548 - accuracy: 0.5927 - val_loss: 0.0428 - val_accuracy: 0.6867\n",
            "Epoch 86/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0548 - accuracy: 0.5934 - val_loss: 0.0434 - val_accuracy: 0.6825\n",
            "Epoch 87/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0544 - accuracy: 0.5973 - val_loss: 0.0442 - val_accuracy: 0.6764\n",
            "Epoch 88/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0546 - accuracy: 0.5960 - val_loss: 0.0441 - val_accuracy: 0.6793\n",
            "Epoch 89/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0540 - accuracy: 0.6005 - val_loss: 0.0441 - val_accuracy: 0.6789\n",
            "Epoch 90/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0539 - accuracy: 0.5998 - val_loss: 0.0449 - val_accuracy: 0.6712\n",
            "Epoch 91/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0537 - accuracy: 0.6017 - val_loss: 0.0427 - val_accuracy: 0.6853\n",
            "Epoch 92/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0536 - accuracy: 0.6031 - val_loss: 0.0448 - val_accuracy: 0.6706\n",
            "Epoch 93/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0538 - accuracy: 0.6020 - val_loss: 0.0435 - val_accuracy: 0.6820\n",
            "Epoch 94/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0536 - accuracy: 0.6006 - val_loss: 0.0427 - val_accuracy: 0.6881\n",
            "Epoch 95/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0528 - accuracy: 0.6087 - val_loss: 0.0416 - val_accuracy: 0.6972\n",
            "Epoch 96/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0529 - accuracy: 0.6114 - val_loss: 0.0421 - val_accuracy: 0.6934\n",
            "Epoch 97/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0532 - accuracy: 0.6065 - val_loss: 0.0415 - val_accuracy: 0.6969\n",
            "Epoch 98/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0529 - accuracy: 0.6091 - val_loss: 0.0419 - val_accuracy: 0.6939\n",
            "Epoch 99/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0527 - accuracy: 0.6122 - val_loss: 0.0404 - val_accuracy: 0.7067\n",
            "Epoch 100/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0527 - accuracy: 0.6116 - val_loss: 0.0428 - val_accuracy: 0.6881\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcBv7Omih4Mx",
        "outputId": "87597a38-ae6f-4448-cff3-7b011ac1963a"
      },
      "source": [
        "model1 = Sequential()\n",
        "model1.add(LSTM(512, batch_input_shape = (None, None, x.shape[2]),return_sequences=True))\n",
        "model1.add(BatchNormalization())\n",
        "model1.add(Dropout(0.3))\n",
        "\n",
        "\n",
        "model1.add(LSTM(256,activation=\"relu\",return_sequences=True))\n",
        "model1.add(BatchNormalization())\n",
        "model1.add(Dropout(0.5))\n",
        "\n",
        "\n",
        "model1.add(LSTM(128,activation=\"relu\",return_sequences=True))\n",
        "model1.add(BatchNormalization())\n",
        "model1.add(Dropout(0.3))\n",
        "\n",
        "model1.add(LSTM(64,activation=\"relu\",return_sequences=True))\n",
        "model1.add(BatchNormalization())\n",
        "model1.add(Dropout(0.3))\n",
        "\n",
        "\n",
        "model1.add(LSTM(32,activation=\"relu\"))\n",
        "model1.add(BatchNormalization())\n",
        "model1.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model1.add(Dense(10))\n",
        "model1.add(Activation('softmax'))\n",
        "\n",
        "rmsprop =keras.optimizers.RMSprop(lr=0.0001, rho=0.9, epsilon=1e-08)\n",
        "model1.compile(loss='mean_squared_error',\n",
        "                  optimizer=rmsprop,\n",
        "                  metrics=['accuracy'])\n",
        "model1.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_5 (LSTM)                (None, None, 512)         1193984   \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, None, 512)         2048      \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, None, 512)         0         \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (None, None, 256)         787456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, None, 256)         1024      \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, None, 256)         0         \n",
            "_________________________________________________________________\n",
            "lstm_7 (LSTM)                (None, None, 128)         197120    \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, None, 128)         512       \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, None, 128)         0         \n",
            "_________________________________________________________________\n",
            "lstm_8 (LSTM)                (None, None, 64)          49408     \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, None, 64)          256       \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, None, 64)          0         \n",
            "_________________________________________________________________\n",
            "lstm_9 (LSTM)                (None, 32)                12416     \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                330       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 2,244,682\n",
            "Trainable params: 2,242,698\n",
            "Non-trainable params: 1,984\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNwWm6IhuMEv"
      },
      "source": [
        "x_train, x_test, y_train1, y_test1 = train_test_split(x, y_aro, test_size = 0.2, random_state = 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRuQGEYjuWeu",
        "outputId": "4785e847-b4fb-4427-d105-efe258fe23ef"
      },
      "source": [
        "history1 =model1.fit(x_train, y_train1, epochs = 100, batch_size=150,validation_data= (x_test, y_test1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "508/508 [==============================] - 17s 22ms/step - loss: 0.0977 - accuracy: 0.1348 - val_loss: 0.0891 - val_accuracy: 0.1444\n",
            "Epoch 2/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0916 - accuracy: 0.2023 - val_loss: 0.0841 - val_accuracy: 0.2756\n",
            "Epoch 3/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0887 - accuracy: 0.2300 - val_loss: 0.0834 - val_accuracy: 0.2834\n",
            "Epoch 4/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0873 - accuracy: 0.2375 - val_loss: 0.0832 - val_accuracy: 0.2843\n",
            "Epoch 5/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0862 - accuracy: 0.2492 - val_loss: 0.0826 - val_accuracy: 0.2914\n",
            "Epoch 6/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0856 - accuracy: 0.2551 - val_loss: 0.0822 - val_accuracy: 0.2945\n",
            "Epoch 7/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0846 - accuracy: 0.2646 - val_loss: 0.0819 - val_accuracy: 0.2939\n",
            "Epoch 8/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0843 - accuracy: 0.2684 - val_loss: 0.0815 - val_accuracy: 0.2991\n",
            "Epoch 9/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0838 - accuracy: 0.2744 - val_loss: 0.0812 - val_accuracy: 0.3035\n",
            "Epoch 10/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0834 - accuracy: 0.2785 - val_loss: 0.0807 - val_accuracy: 0.3118\n",
            "Epoch 11/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0827 - accuracy: 0.2862 - val_loss: 0.0803 - val_accuracy: 0.3200\n",
            "Epoch 12/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0824 - accuracy: 0.2915 - val_loss: 0.0797 - val_accuracy: 0.3227\n",
            "Epoch 13/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0820 - accuracy: 0.2952 - val_loss: 0.0793 - val_accuracy: 0.3255\n",
            "Epoch 14/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0814 - accuracy: 0.3019 - val_loss: 0.0790 - val_accuracy: 0.3243\n",
            "Epoch 15/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0810 - accuracy: 0.3088 - val_loss: 0.0783 - val_accuracy: 0.3310\n",
            "Epoch 16/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0808 - accuracy: 0.3116 - val_loss: 0.0781 - val_accuracy: 0.3338\n",
            "Epoch 17/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0804 - accuracy: 0.3123 - val_loss: 0.0776 - val_accuracy: 0.3386\n",
            "Epoch 18/100\n",
            "508/508 [==============================] - 10s 19ms/step - loss: 0.0801 - accuracy: 0.3158 - val_loss: 0.0774 - val_accuracy: 0.3410\n",
            "Epoch 19/100\n",
            "508/508 [==============================] - 10s 19ms/step - loss: 0.0797 - accuracy: 0.3229 - val_loss: 0.0771 - val_accuracy: 0.3437\n",
            "Epoch 20/100\n",
            "508/508 [==============================] - 10s 19ms/step - loss: 0.0795 - accuracy: 0.3257 - val_loss: 0.0766 - val_accuracy: 0.3555\n",
            "Epoch 21/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0793 - accuracy: 0.3273 - val_loss: 0.0763 - val_accuracy: 0.3561\n",
            "Epoch 22/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0788 - accuracy: 0.3317 - val_loss: 0.0758 - val_accuracy: 0.3617\n",
            "Epoch 23/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0785 - accuracy: 0.3373 - val_loss: 0.0755 - val_accuracy: 0.3639\n",
            "Epoch 24/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0784 - accuracy: 0.3370 - val_loss: 0.0750 - val_accuracy: 0.3717\n",
            "Epoch 25/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0779 - accuracy: 0.3431 - val_loss: 0.0745 - val_accuracy: 0.3775\n",
            "Epoch 26/100\n",
            "508/508 [==============================] - 10s 19ms/step - loss: 0.0781 - accuracy: 0.3412 - val_loss: 0.0744 - val_accuracy: 0.3797\n",
            "Epoch 27/100\n",
            "508/508 [==============================] - 10s 19ms/step - loss: 0.0776 - accuracy: 0.3494 - val_loss: 0.0740 - val_accuracy: 0.3815\n",
            "Epoch 28/100\n",
            "508/508 [==============================] - 10s 19ms/step - loss: 0.0773 - accuracy: 0.3518 - val_loss: 0.0735 - val_accuracy: 0.3878\n",
            "Epoch 29/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0772 - accuracy: 0.3511 - val_loss: 0.0734 - val_accuracy: 0.3929\n",
            "Epoch 30/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0768 - accuracy: 0.3546 - val_loss: 0.0733 - val_accuracy: 0.3974\n",
            "Epoch 31/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0765 - accuracy: 0.3631 - val_loss: 0.0727 - val_accuracy: 0.3948\n",
            "Epoch 32/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0765 - accuracy: 0.3639 - val_loss: 0.0723 - val_accuracy: 0.4032\n",
            "Epoch 33/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0762 - accuracy: 0.3674 - val_loss: 0.0721 - val_accuracy: 0.4079\n",
            "Epoch 34/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0758 - accuracy: 0.3710 - val_loss: 0.0717 - val_accuracy: 0.4030\n",
            "Epoch 35/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0757 - accuracy: 0.3676 - val_loss: 0.0720 - val_accuracy: 0.4049\n",
            "Epoch 36/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0753 - accuracy: 0.3755 - val_loss: 0.0715 - val_accuracy: 0.4051\n",
            "Epoch 37/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0753 - accuracy: 0.3751 - val_loss: 0.0711 - val_accuracy: 0.4155\n",
            "Epoch 38/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0751 - accuracy: 0.3766 - val_loss: 0.0706 - val_accuracy: 0.4190\n",
            "Epoch 39/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0747 - accuracy: 0.3793 - val_loss: 0.0706 - val_accuracy: 0.4221\n",
            "Epoch 40/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0746 - accuracy: 0.3868 - val_loss: 0.0704 - val_accuracy: 0.4209\n",
            "Epoch 41/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0744 - accuracy: 0.3868 - val_loss: 0.0697 - val_accuracy: 0.4283\n",
            "Epoch 42/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0745 - accuracy: 0.3855 - val_loss: 0.0695 - val_accuracy: 0.4351\n",
            "Epoch 43/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0740 - accuracy: 0.3918 - val_loss: 0.0693 - val_accuracy: 0.4322\n",
            "Epoch 44/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0736 - accuracy: 0.3947 - val_loss: 0.0692 - val_accuracy: 0.4367\n",
            "Epoch 45/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0737 - accuracy: 0.3936 - val_loss: 0.0685 - val_accuracy: 0.4444\n",
            "Epoch 46/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0734 - accuracy: 0.4001 - val_loss: 0.0685 - val_accuracy: 0.4471\n",
            "Epoch 47/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0733 - accuracy: 0.4000 - val_loss: 0.0685 - val_accuracy: 0.4427\n",
            "Epoch 48/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0733 - accuracy: 0.4011 - val_loss: 0.0679 - val_accuracy: 0.4502\n",
            "Epoch 49/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0730 - accuracy: 0.4036 - val_loss: 0.0687 - val_accuracy: 0.4439\n",
            "Epoch 50/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0728 - accuracy: 0.4053 - val_loss: 0.0680 - val_accuracy: 0.4492\n",
            "Epoch 51/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0726 - accuracy: 0.4079 - val_loss: 0.0674 - val_accuracy: 0.4548\n",
            "Epoch 52/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0724 - accuracy: 0.4102 - val_loss: 0.0678 - val_accuracy: 0.4484\n",
            "Epoch 53/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0726 - accuracy: 0.4049 - val_loss: 0.0672 - val_accuracy: 0.4593\n",
            "Epoch 54/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0724 - accuracy: 0.4079 - val_loss: 0.0667 - val_accuracy: 0.4613\n",
            "Epoch 55/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0721 - accuracy: 0.4126 - val_loss: 0.0665 - val_accuracy: 0.4635\n",
            "Epoch 56/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0717 - accuracy: 0.4172 - val_loss: 0.0665 - val_accuracy: 0.4659\n",
            "Epoch 57/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0717 - accuracy: 0.4173 - val_loss: 0.0661 - val_accuracy: 0.4634\n",
            "Epoch 58/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0714 - accuracy: 0.4217 - val_loss: 0.0658 - val_accuracy: 0.4717\n",
            "Epoch 59/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0717 - accuracy: 0.4185 - val_loss: 0.0655 - val_accuracy: 0.4759\n",
            "Epoch 60/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0712 - accuracy: 0.4234 - val_loss: 0.0659 - val_accuracy: 0.4625\n",
            "Epoch 61/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0708 - accuracy: 0.4249 - val_loss: 0.0651 - val_accuracy: 0.4752\n",
            "Epoch 62/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0711 - accuracy: 0.4228 - val_loss: 0.0655 - val_accuracy: 0.4757\n",
            "Epoch 63/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0707 - accuracy: 0.4298 - val_loss: 0.0652 - val_accuracy: 0.4732\n",
            "Epoch 64/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0705 - accuracy: 0.4315 - val_loss: 0.0646 - val_accuracy: 0.4796\n",
            "Epoch 65/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0705 - accuracy: 0.4316 - val_loss: 0.0642 - val_accuracy: 0.4851\n",
            "Epoch 66/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0705 - accuracy: 0.4337 - val_loss: 0.0641 - val_accuracy: 0.4864\n",
            "Epoch 67/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0701 - accuracy: 0.4340 - val_loss: 0.0639 - val_accuracy: 0.4862\n",
            "Epoch 68/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0701 - accuracy: 0.4350 - val_loss: 0.0641 - val_accuracy: 0.4878\n",
            "Epoch 69/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0699 - accuracy: 0.4386 - val_loss: 0.0639 - val_accuracy: 0.4886\n",
            "Epoch 70/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0699 - accuracy: 0.4379 - val_loss: 0.0638 - val_accuracy: 0.4905\n",
            "Epoch 71/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0696 - accuracy: 0.4387 - val_loss: 0.0632 - val_accuracy: 0.4940\n",
            "Epoch 72/100\n",
            "508/508 [==============================] - 10s 21ms/step - loss: 0.0692 - accuracy: 0.4441 - val_loss: 0.0629 - val_accuracy: 0.4990\n",
            "Epoch 73/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0693 - accuracy: 0.4449 - val_loss: 0.0627 - val_accuracy: 0.5009\n",
            "Epoch 74/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0694 - accuracy: 0.4399 - val_loss: 0.0636 - val_accuracy: 0.4906\n",
            "Epoch 75/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0694 - accuracy: 0.4424 - val_loss: 0.0631 - val_accuracy: 0.4946\n",
            "Epoch 76/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0691 - accuracy: 0.4479 - val_loss: 0.0627 - val_accuracy: 0.4975\n",
            "Epoch 77/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0690 - accuracy: 0.4482 - val_loss: 0.0622 - val_accuracy: 0.5027\n",
            "Epoch 78/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0688 - accuracy: 0.4491 - val_loss: 0.0629 - val_accuracy: 0.4960\n",
            "Epoch 79/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0687 - accuracy: 0.4513 - val_loss: 0.0618 - val_accuracy: 0.5077\n",
            "Epoch 80/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0683 - accuracy: 0.4531 - val_loss: 0.0616 - val_accuracy: 0.5112\n",
            "Epoch 81/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0682 - accuracy: 0.4542 - val_loss: 0.0619 - val_accuracy: 0.5056\n",
            "Epoch 82/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0681 - accuracy: 0.4564 - val_loss: 0.0614 - val_accuracy: 0.5123\n",
            "Epoch 83/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0683 - accuracy: 0.4549 - val_loss: 0.0616 - val_accuracy: 0.5106\n",
            "Epoch 84/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0679 - accuracy: 0.4580 - val_loss: 0.0613 - val_accuracy: 0.5152\n",
            "Epoch 85/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0680 - accuracy: 0.4564 - val_loss: 0.0615 - val_accuracy: 0.5105\n",
            "Epoch 86/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0676 - accuracy: 0.4610 - val_loss: 0.0609 - val_accuracy: 0.5178\n",
            "Epoch 87/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0678 - accuracy: 0.4603 - val_loss: 0.0606 - val_accuracy: 0.5199\n",
            "Epoch 88/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0676 - accuracy: 0.4631 - val_loss: 0.0607 - val_accuracy: 0.5181\n",
            "Epoch 89/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0674 - accuracy: 0.4633 - val_loss: 0.0610 - val_accuracy: 0.5205\n",
            "Epoch 90/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0674 - accuracy: 0.4639 - val_loss: 0.0599 - val_accuracy: 0.5291\n",
            "Epoch 91/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0671 - accuracy: 0.4669 - val_loss: 0.0596 - val_accuracy: 0.5327\n",
            "Epoch 92/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0673 - accuracy: 0.4651 - val_loss: 0.0595 - val_accuracy: 0.5327\n",
            "Epoch 93/100\n",
            "508/508 [==============================] - 10s 21ms/step - loss: 0.0674 - accuracy: 0.4622 - val_loss: 0.0593 - val_accuracy: 0.5365\n",
            "Epoch 94/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0674 - accuracy: 0.4634 - val_loss: 0.0594 - val_accuracy: 0.5330\n",
            "Epoch 95/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0668 - accuracy: 0.4712 - val_loss: 0.0596 - val_accuracy: 0.5312\n",
            "Epoch 96/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0670 - accuracy: 0.4675 - val_loss: 0.0595 - val_accuracy: 0.5333\n",
            "Epoch 97/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0668 - accuracy: 0.4722 - val_loss: 0.0589 - val_accuracy: 0.5385\n",
            "Epoch 98/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0666 - accuracy: 0.4707 - val_loss: 0.0587 - val_accuracy: 0.5448\n",
            "Epoch 99/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0661 - accuracy: 0.4757 - val_loss: 0.0592 - val_accuracy: 0.5343\n",
            "Epoch 100/100\n",
            "508/508 [==============================] - 10s 20ms/step - loss: 0.0663 - accuracy: 0.4758 - val_loss: 0.0587 - val_accuracy: 0.5408\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h99Qu4saVxiw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwUJsWFdVxft"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fW2bQIO2VxdI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMjTfC7cVxah"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubgZZhf1VxXi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIW6SJVAVxU_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxsw_sLgVxR9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty8Bi0XUVxPI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVAoBETXVxLu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJw6a1-pVxG6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DggXChsLVw8i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3crOrDpSwK8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "16aab287-af10-47b1-c064-ac3362181f52"
      },
      "source": [
        "loss_train = model['train_loss']\n",
        "loss_val = model['val_loss']\n",
        "epochs = range(1,100)\n",
        "plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
        "plt.plot(epochs, loss_val, 'b', label='validation loss')\n",
        "plt.title('Training and Validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-c20e48ac9b36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mloss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'g'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'validation loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'Sequential' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PkRjOjVwLtv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGztmht6wLd5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ucq7_20SHSg9"
      },
      "source": [
        "# Testing\n",
        "testingList=['06']\n",
        "for sub in testingList:\n",
        "    getPSD(sub)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TWcoSzyj9ZJ"
      },
      "source": [
        "testingdata= []\n",
        "testinglabel = []\n",
        "for subjects in testingList:\n",
        "  \n",
        "\n",
        "    with open('/content/out\\s' + subjects + '.npy', 'rb') as file:\n",
        "        testingsub = np.load(file,allow_pickle=True)\n",
        "        for i in range (0,testingsub.shape[0]):\n",
        "          testingdata.append(testingsub[i][0])\n",
        "          testinglabel.append(testingsub[i][1])\n",
        "np.save('testingdata', np.array(testingdata), allow_pickle=True, fix_imports=True)\n",
        "np.save('testinglabel', np.array(testinglabel), allow_pickle=True, fix_imports=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtRqwEjFkghP"
      },
      "source": [
        "df_T=pd.DataFrame(data=testingdata)\n",
        "df_T.to_csv(\"testingdata.csv\",index=False)\n",
        "\n",
        "df1_T=pd.DataFrame(data=testinglabel)\n",
        "df1_T.to_csv(\"testinglabel.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "Mmz2IoDBkyli",
        "outputId": "914441ec-bd46-408d-c97e-b09e863394d6"
      },
      "source": [
        "dataT=pd.read_csv(\"testingdata.csv\")\n",
        "dataT\n",
        "labelT=pd.read_csv(\"testinglabel.csv\")\n",
        "labelT"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6.83</td>\n",
              "      <td>6.62</td>\n",
              "      <td>7.08</td>\n",
              "      <td>5.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6.83</td>\n",
              "      <td>6.62</td>\n",
              "      <td>7.08</td>\n",
              "      <td>5.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6.83</td>\n",
              "      <td>6.62</td>\n",
              "      <td>7.08</td>\n",
              "      <td>5.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6.83</td>\n",
              "      <td>6.62</td>\n",
              "      <td>7.08</td>\n",
              "      <td>5.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6.83</td>\n",
              "      <td>6.62</td>\n",
              "      <td>7.08</td>\n",
              "      <td>5.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19027</th>\n",
              "      <td>4.85</td>\n",
              "      <td>4.64</td>\n",
              "      <td>5.04</td>\n",
              "      <td>5.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19028</th>\n",
              "      <td>4.85</td>\n",
              "      <td>4.64</td>\n",
              "      <td>5.04</td>\n",
              "      <td>5.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19029</th>\n",
              "      <td>4.85</td>\n",
              "      <td>4.64</td>\n",
              "      <td>5.04</td>\n",
              "      <td>5.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19030</th>\n",
              "      <td>4.85</td>\n",
              "      <td>4.64</td>\n",
              "      <td>5.04</td>\n",
              "      <td>5.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19031</th>\n",
              "      <td>4.85</td>\n",
              "      <td>4.64</td>\n",
              "      <td>5.04</td>\n",
              "      <td>5.03</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>19032 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0     1     2     3\n",
              "0      6.83  6.62  7.08  5.96\n",
              "1      6.83  6.62  7.08  5.96\n",
              "2      6.83  6.62  7.08  5.96\n",
              "3      6.83  6.62  7.08  5.96\n",
              "4      6.83  6.62  7.08  5.96\n",
              "...     ...   ...   ...   ...\n",
              "19027  4.85  4.64  5.04  5.03\n",
              "19028  4.85  4.64  5.04  5.03\n",
              "19029  4.85  4.64  5.04  5.03\n",
              "19030  4.85  4.64  5.04  5.03\n",
              "19031  4.85  4.64  5.04  5.03\n",
              "\n",
              "[19032 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgfFQuNMlFqU",
        "outputId": "977fe157-7c2c-4e30-c63d-289d3888766c"
      },
      "source": [
        "y_valenceT=labelT.loc[:,'0']\n",
        "y_valenceT\n",
        "y_arousalT=labelT.loc[:,'1']\n",
        "y_arousalT"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        6.62\n",
              "1        6.62\n",
              "2        6.62\n",
              "3        6.62\n",
              "4        6.62\n",
              "         ... \n",
              "19027    4.64\n",
              "19028    4.64\n",
              "19029    4.64\n",
              "19030    4.64\n",
              "19031    4.64\n",
              "Name: 1, Length: 19032, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0J0QhwhlRA7",
        "outputId": "5afea2a9-f3b9-410b-84a3-c55c90373d1b"
      },
      "source": [
        "xT=dataT\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(xT)\n",
        "xT = scaler.transform(xT)\n",
        "print(xT)\n",
        "print(xT.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.08847128  1.20616467 -1.24902333 ...  0.62095014  1.4805994\n",
            "  -1.86963798]\n",
            " [ 0.08400614  1.15300788 -1.28895029 ... -0.2885883   1.02144828\n",
            "  -1.82295674]\n",
            " [ 0.06916316  1.38498115 -1.2367197  ...  2.09825543  1.42659651\n",
            "  -1.31130107]\n",
            " ...\n",
            " [-0.76585177  1.49328679 -1.39394934 ... -1.43513067 -0.11022959\n",
            "   0.89263428]\n",
            " [-0.78125058  1.24862363 -1.17441051 ... -1.10172243  0.0945205\n",
            "   1.40821624]\n",
            " [-0.82275298  1.19349525 -1.38219126 ... -0.13491677 -0.66477127\n",
            "   1.18817111]]\n",
            "(19032, 70)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wD-I_BCs2Kb3"
      },
      "source": [
        "xT = np.reshape(xT, (xT.shape[0],1,xT.shape[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUKsbnz7zqhL",
        "outputId": "e026def2-4320-4d96-c9bc-0b7a639f6e24"
      },
      "source": [
        "predictions = model.predict(xT,batch_size=150 )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 38.21017074584961\n",
            "Test accuracy: 0.6727616786956787\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vSHqyAFjnVu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}